{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gestion du projet**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tous mes restes à faire (évolutions)\n",
    "\n",
    "A l'issue de la soutenance du 2 avril 2023.\n",
    "\n",
    "* dataxploring :\n",
    "    * Virer tout ce qui est en doublon avec cats_extract et mettre un renvoi (lien Github).\n",
    "* im_prep :\n",
    "    * revoir section fonctionnalités / sauvegarde et mise en table des descripteurs : il y a du code déprécié qui plante.\n",
    "    * scinder en deux, avec la partie découverte et fonctionnalités de base, et la partie pipeline\n",
    "* Nom des sous-dossiers d'images marquées dans sample_8 et sample_100 : terminer de nettoyer et d'uniformiser\n",
    "* Im CNN : terminer\n",
    "* Analyse de corpus :\n",
    "    * Corpus 'specs' pour voir.\n",
    "    * Analyse fine par classe => prétraitement modulé suivant la classe\n",
    "        * Détails *Raffiner par classe réelle l’étude des corpus : par exemple, sur Home Furnishing, on peut chercher à réduire le bruit : listes ciblées de stop words, pondération du niveau de réglage suivant les classes : on voit bien que Home Furnishing est en pleine confusion : des articles dans toutes les classes, y compris la très bien cernée Watches (4 cas) sont classées à tort dans Home Furnishing.*\n",
    "* Finitions :\n",
    "    * Partout et d'abord dans tx_pipeline : sents -> corpus\n",
    "    * Code, revue des typehints et docstrings, revue des pbs pylance\n",
    "    * Revoir et compléter la prez : tableau dense des résulats cf. algo + schéma dense de ma pipeline.\n",
    "    * Terminer de reprendre l'exemple du séminaire im avec mes propres fonctions -> c'est un Kaggle, donc un bonne perf serait bien pour la visibilité\n",
    "    * virer plot.py s'il reste vide, uniformiser les fonctions de chargement et sauvegarde, un peu dispersées dans les modules -> un seul io.py de référence.\n",
    "    * Finaliser la documentation des fonctions du README, puis le README (synthèse d'ensemble)\n",
    "* Sauvegarde de configs : attributs et params\n",
    "* Pipeline :\n",
    "    * la cellule 'tout générer from scratch'.\n",
    "    * Convergence des pipelines tx et im\n",
    "    * Combinaison des prédiction (cf. forêts aléatoires) => un 80 à 90 % est atteignable\n",
    "        * avec vote pondéré cf. les classes d'après les scores détaillés (P, R, F1) des différents algos\n",
    "    * tx_pipeline : terminer d'intégrer le preprocessing du corpus avec ses paramètres\n",
    "    * intégration de l'échantillonage stratifié d'un sous-ensemble d'apprentissage\n",
    "    * intégration de la distinction entre ensemble d'apprentissage et ensemble de prédiction\n",
    "* Word2Vec\n",
    "    * Reste l'iput layer étrange\n",
    "* BERT\n",
    "    * Comparer avec et sans la lemmatisation du corpus (la lemmatisation et stemmatisation sont intéressantes pour les count et freq vectorizers, mais pas pour les ambeddings qui perdent alors de l'information sémantique)\n",
    "    * Comment trouver le meilleur modèle pré-entrainé compte tenu de la nature du projet ?\n",
    "* Réduction de dimension : tester tous les autres, en commençant par kTSNE.\n",
    "* Classification finale (tSNE > kMeans) :\n",
    "    * Revoir l'intégration de la méthode du coude : la solution empirique de la racine comme mon elbow maison n'ont pas été satisfaisants\n",
    "    * Essayer d'autres classifieurs (en fait tous les autres de SKL en commençant par logistique, forêt et SVM)\n",
    "    * Combinaison les prédictions concurrentes : techniques d'apprentissage ensembliste telles que le Bagging, le Boosting ou le Stacking. Ces techniques consistent à entraîner plusieurs modèles différents et à combiner leurs prédictions pour améliorer les performances globales du modèle.\n",
    "\n",
    "* Bench / gridsearch\n",
    "    * Poursuivre, notamment sur BERT quand il est réparé\n",
    "    * Lancer mes premiers gridsearchs à deux paramètres liés, puis 3, etc\n",
    "    * Voir effet de la réduction de la base d'apprentissage à un échantillon stratifié\n",
    "        * détails *Intégrer la réduction de l’ensemble d’apprentissage à un sample stratifié et voir l’effet sur les perfs selon le % : c’est un bench comme les autres.*\n",
    "    * gridsearch : inrégrer les autres blocs de paramètres (que ex_params pour le moment)\n",
    "    * Confort : production de tableau de synthèse directement en version markdown\n",
    "    * Confort++ : générer une version animée de l'évolution des constellations tSNE cf. l'évolution des paramètres de gridsearch\n",
    "* Embeddings : tester GPT-3, 4 et T5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Milestone 1** : Faisabilité de classification – Texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conseils\n",
    "\n",
    "**Livrable** : Notebook d’analyse de faisabilité de classification automatique des produits par leur description textuelle.\n",
    "\n",
    "**Charge** : 30 %\n",
    "\n",
    "**Problèmes et erreurs courants** :\n",
    "* ⚠ L’objectif est de vérifier la faisabilité de classifier automatiquement les produits, simplement via une représentation en 2D des produits et une vérification d’une séparation automatique selon leur catégorie réelle (classification non supervisée). Il n’est donc pas demandé de réaliser de classification supervisée de prédiction des catégories des produits.\n",
    "\n",
    "**Recommandations** :\n",
    "* Réaliser un prétraitement de texte : nettoyage (ponctuation, stopwords…), lemmatization…\n",
    "* Ce prétraitement doit être adapté au contexte et surtout à l’objectif. Nous cherchons à séparer les produits selon leur description textuelle. Les noms, voire certains verbes, permettent de décrire ces produits. Les adjectifs ou les adverbes sont beaucoup moins pertinents.\n",
    "* Réaliser un bag of words (countVectorizer, Tf-idf…) afin de créer des « features » pour chaque produit.\n",
    "Vous pourrez prendre la variable « description » et la variable « product_name » qui contient les principaux mots qui décrivent un produit.\n",
    "* Pour le bag of words, l’étudiant pourra tester plusieurs approches : par exemple fit et transform sur « description » ou sur « product_name » + « description », fit sur « product_name » et transform sur « product_name » + « description » (permet de ne garder que le vocabulaire des « product_name » moins verbeux, et de renforcer le comptage avec le contenu de « description ».\n",
    "* Réaliser une réduction de dimension via ACP. Une approche complémentaire via LDA afin de créer des features de dimension réduite peut être testée, elle n’est pas obligatoire.\n",
    "* Comme présenté dans le webinaire sur le traitement d’images (cf. onglet Ressources), réaliser un T-SNE afin de réduire à 2 composantes les features, et les afficher en coloriant selon la catégorie réelle (1er niveau de « product_category_tree » = 7 catégories, 150 produits par catégorie).\n",
    "* Le graphique montrera clairement que les produits sont relativement bien séparés selon les catégories réelles.\n",
    "* Afin de vérifier l’aspect visuel, réaliser un clustering k-means à partir des 2 composantes du T-SNE (7 clusters comme le nombre de catégories), afficher les 2 composantes du T-SNE en coloriant selon le numéro de cluster du k-means, et comparer la similarité de la catégorisation (catégorie réelle / cluster k-means) via l’adjusted Rand Score (ARI). La valeur, de l’ordre de 0.4 à 0.5, confirme le visuel et donc la faisabilité de classer automatiquement les produits.\n",
    "* En option, l’étudiant pourra analyser plus finement par sous-catégories.\n",
    "\n",
    "**Ressources** :\n",
    "* ARI : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Milestone 2** : Faisabilité de classification – Text Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conseils\n",
    "\n",
    "**Livrable** : Notebook complémentaire au précédent, mettant en œuvre des techniques d’embedding.\n",
    "\n",
    "**Charge** : 15 %\n",
    "\n",
    "**Problèmes et erreurs courants** :\n",
    "* ⚠ Attention à ne pas passer trop de temps sur ce sujet ;).\n",
    "\n",
    "**Recommandations** :\n",
    "* L’objectif de cette étape est de permettre de découvrir des techniques NLP plus avancées.\n",
    "* Il existe, dans les ressources, un notebook donnant un exemple de mise en œuvre de ces techniques : Word2Vec (ou remplacé par Doc2Vec), BERT, USE (Universal Sentence Encoder).\n",
    "* Tu réaliseras la création de features à l’aide de chacune de ces trois techniques (technique de « feature extraction » orientée « sentence embedding »). Il n’est pas attendu une grande expertise, il s’agit surtout d’une introduction à ces techniques.\n",
    "* L’analyse graphique T-SNE et le calcul de l’ARI permettront de comparer les résultats avec les techniques plus simples de CountVectorizer ou Tf-idf.\n",
    "\n",
    "**Ressources** :\n",
    "* Notebook – Exemple de Sentence Embedding : cf. Ressources.\n",
    "* Word/sentence Embedding BERT – Hugging Face : Exemple de Word Embedding BERT via Hugging Face.\n",
    "* Word/sentence Embedding BERT – Hub pour  TensorFlow : exemple de Word Embedding BERT via le hub TensorFlow\n",
    "* Sentence Embedding USE :  Exemple de Sentence Embedding USE (Universal Sentence Encoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Milestone 3** : Faisabilité de classification automatique d’images via SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conseils\n",
    "\n",
    "**Livrable** : Notebook d’analyse de faisabilité de classification automatique d’images via SIFT, ORB ou SURF\n",
    "\n",
    "**Niveau d’avancement** : 30 %\n",
    "\n",
    "**Problèmes et erreurs courants** :\n",
    "* ⚠ L’objectif est de vérifier la faisabilité de classifier automatiquement les images, simplement via une représentation en 2D des images et une vérification d’une séparation automatique selon leur catégorie réelle (classification non supervisée). Il n’est donc pas demandé de réaliser de classification supervisée de prédiction de catégories des images.\n",
    "\n",
    "**Recommandations** :\n",
    "* Le webinaire sur le traitement d’images (cf. onglet Ressources) indique dans le détail la réalisation d’extraction de features et l’affichage via T-SNE, afin de vérifier la séparation automatique de manière non supervisée des images par catégorie (classification non supervisée).\n",
    "* Réaliser dans un premier temps une analyse d’une image et différentes approches de transformation : niveaux de gris, equalization, filtrage bruit, contraste, floutage… (affichage image et histogramme associés).\n",
    "* Ensuite réaliser l’extraction des descripteurs (cf. webinaire).\n",
    "Puis générer les « features » des images via un bag of virtual words (création de clusters de descripteurs et comptage pour chaque image du nombre de descripteurs par cluster).\n",
    "* Réaliser une réduction de dimension (ACP).\n",
    "* Réaliser un T-SNE afin de réduire à 2 composantes les features, et les afficher en coloriant selon la catégorie réelle.\n",
    "* Il est assez difficile de séparer les images selon le label. Le résultat n’est donc pas très concluant avec SIFT.\n",
    "* Afin de vérifier l’aspect visuel, réaliser un clustering k-means à partir des 2 composantes du T-SNE (7 clusters comme le nombre de catégories réelles), afficher les 2 composantes du T-SNE en coloriant selon le numéro de cluster du k-means, et comparer la similarité de la catégorisation (catégorie réelle / cluster k-means) via l’Adjusted Rand Score (ARI). La valeur, de l’ordre de 0.05 à 0.1, confirme le visuel.\n",
    "\n",
    "**Ressources** :\n",
    "* ARI : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Milestone 4** : Faisabilité de classification automatique d’images via CNN Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conseils\n",
    "\n",
    "**Livrable** : Notebook d’analyse de faisabilité de classification automatique d’images via CNN Transfer Learning.\n",
    "\n",
    "**Charge** : 25 %\n",
    "\n",
    "**Problèmes et erreurs courants** :\n",
    "* cf. milestone 3 (SIFT)\n",
    "\n",
    "**Recommandations** :\n",
    "* Récupérer un modèle préentraîné comme précisé dans la ressource « Transfer Learning in Keras with Computer Vision Models », en particulier le paragraphe « Pre-Trained Model as Feature Extractor Preprocessor ».\n",
    "* La suite est identique au milestone 3 (SIFT) :\n",
    "* ACP, T-SNE, k-means, affichage des 2 composantes T-SNE des images coloriées selon la catégorie réelle, puis selon le numéro de cluster, calcul ARI.\n",
    "* Le résultat tant visuel que calculé (0.4 à 0.6) est bien plus pertinent et montre, sans entraînement d’un modèle, la faisabilité de réaliser une classification automatique.\n",
    "\n",
    "**Ressources** :\n",
    "* Transfer Learning in Keras with Computer Vision Models : https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
