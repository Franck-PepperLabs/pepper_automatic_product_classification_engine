{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploration de corpus variés**\n",
    "\n",
    "Quelques cas d'applications pour se faire la main :\n",
    "* Le cas proposé (paroles de chansons) à faire from scratch pour éviter de se sentir bête\n",
    "* Articles de loi sur Légifrance\n",
    "* Définitions du CNRTL\n",
    "* Le corpus autour de la data sur Wikipedia EN\n",
    "* Le contenu des cours d'OC\n",
    "* Des livres complets que l'on peut récupérer en ligne, comme Le Discours de la Méthode, ou Récolte et Semailles.\n",
    "* L'exemple Kaggle du TP qui arrive dans deux chapitres\n",
    "\n",
    "Voir également les exemples de NLTK\n",
    "\n",
    "\n",
    "Kaggle : https://www.kaggle.com/competitions/nlp-getting-started/data Disaster Tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les corpus de NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Corpus Gutenberg\n",
    "\n",
    "NLTK inclut une petite sélection de textes provenant de l'archive électronique Project Gutenberg, qui contient environ 25 000 livres électroniques gratuits, hébergés à l'adresse http://www.gutenberg.org/. Nous commençons par faire charger le package NLTK par l'interpréteur Python, puis nous demandons de voir les identificateurs de fichiers via `nltk.corpus.gutenberg.fileids()` de ce corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
    "emma.concordance(\"surprize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "print(macbeth_sentences)\n",
    "print(macbeth_sentences[1116])\n",
    "longest_len = max(len(s) for s in macbeth_sentences)\n",
    "print(longest_len)\n",
    "print([s for s in macbeth_sentences if len(s) == longest_len])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Texte web et chat\n",
    "\n",
    "Bien que Project Gutenberg contienne des milliers de livres, il représente une littérature établie. Il est important de considérer également un langage moins formel. La petite collection de textes Web de NLTK inclut des contenus provenant d'un forum de discussion Firefox, des conversations entendues à New York, le scénario du film *Pirates des Caraïbes*, des annonces personnelles et des critiques de vin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "# nltk.download('webtext')\n",
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65], '...')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe également un corpus de sessions de messagerie instantanée, collecté à l'origine par la Naval Postgraduate School pour des recherches sur la détection automatique des prédateurs Internet. Le corpus contient plus de 10 000 messages, anonymisés en remplaçant les noms d'utilisateur par des noms génériques de la forme \"UserNNN\" et édités manuellement pour supprimer toute autre information d'identification. Le corpus est organisé en 15 fichiers, où chaque fichier contient plusieurs centaines de messages collectés à une date donnée, pour une salle de chat spécifique à l'âge (ados, 20 ans, 30 ans, 40 ans, plus une salle de chat générique pour adultes). Le nom de fichier contient la date, la salle de chat et le nombre de messages ; par exemple, 10-19-20s_706posts.xml contient 706 messages collectés de la salle de chat 20 ans le 19/10/2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat\n",
    "# nltk.download('nps_chat')\n",
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
    "chatroom[123]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Corpus Brown\n",
    "\n",
    "Le corpus Brown était le premier corpus électronique de million de mots en anglais, créé en 1961 à l'Université Brown. Ce corpus contient des textes provenant de 500 sources et les sources ont été catégorisées par genre, comme les *nouvelles*, les *éditoriaux*, etc. La table 1.1 donne un exemple de chaque genre (pour une liste complète, voir `http://icame.uib.no/brown/bcm-los.html`).\n",
    "\n",
    "**Table 1.1 :**\n",
    "\n",
    "Document d'exemple pour chaque section du corpus Brown\n",
    "\n",
    "|ID|Ficher|Genre|Description|\n",
    "|-|-|-|-|\n",
    "|A16|ca16|news|Chicago Tribune: *Society Reportage*|\n",
    "|B02|cb02|editorial|Christian Science Monitor: *Editorials*|\n",
    "|C17|cc17|reviews|Time Magazine: *Reviews*|\n",
    "|D12|cd12|religion|Underwood: *Probing the Ethics of Realtors*|\n",
    "|E36|ce36|hobbies|Norling: *Renting a Car in Europe*|\n",
    "|F25|cf25|lore|Boroff: *Jewish Teenage Culture*|\n",
    "|G22|cg22|belles_lettres|Reiner: *Coping with Runaway Technology*|\n",
    "|H15|ch15|government|US Office of Civil and Defence Mobilization: *The Family Fallout Shelter*|\n",
    "|J17|cj19|learned|Mosteller: *Probability with Statistical Applications*|\n",
    "|K04|ck04|fiction|W.E.B. Du Bois: *Worlds of Color*|\n",
    "|L13|cl13|mystery|Hitchens: *Footsteps in the Night*|\n",
    "|M01|cm01|science_fiction|Heinlein: *Stranger in a Strange Land*|\n",
    "|N14|cn15|adventure|Field: *Rattlesnake Ridge*|\n",
    "|P12|cp12|romance|Callaghan: *A Passion in Rome*|\n",
    "|R06|cr06|humor|Thurber: *The Future, If Any, of Comedy*|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons accéder au corpus sous forme de liste de mots ou de liste de phrases (où chaque phrase est elle-même une liste de mots). Nous pouvons éventuellement spécifier des catégories ou des fichiers particuliers à lire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "# nltk.download('brown')\n",
    "print(brown.categories())\n",
    "print(brown.words(categories='news'))\n",
    "print(brown.words(fileids=['cg22']))\n",
    "print(brown.sents(categories=['news', 'editorial', 'reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m], end=' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le corpus Brown est une ressource pratique pour étudier les différences systématiques entre les genres, une sorte d'enquête linguistique connue sous le nom de **stylistique**. Comparons les genres dans leur utilisation des verbes modaux. La première étape consiste à produire les comptes pour un genre particulier. N'oubliez pas d'importer nltk avant de faire ce qui suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Corpus Reuters\n",
    "\n",
    "Le **Corpus Reuters** contient 10 788 documents d'actualités totalisant 1,3 million de mots. Les documents ont été classés en 90 sujets et regroupés en deux ensemble appelés \"formation\" et \"test\"; ainsi, le texte avec l'ID de fichier `'test/14826'` est un document tiré de l'ensemble de test. Cette séparation est destinée à entraîner et à tester des algorithmes qui détectent automatiquement le sujet d'un document, comme nous le verrons dans le chapitre [**Apprendre à classer le texte** (6)](https://www.nltk.org/book/ch06.html#chap-data-intensive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "# nltk.download('reuters')\n",
    "display(reuters.fileids()[:10])\n",
    "display(reuters.categories()[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Corpus des discours inauguraux\n",
    "\n",
    "Dans 1, nous avons examiné le Corpus des Discours Inauguraux, mais nous l'avons traité comme un seul texte. Le graphique de fig-inaugural utilisait \"décalage de mot\" comme l'un des axes ; c'est l'index numérique du mot dans le corpus, comptant à partir du premier mot du premier discours. Cependant, le corpus est en réalité une collection de 55 textes, un pour chaque discours présidentiel. Une propriété intéressante de cette collection est sa dimension temporelle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "# nltk.download('inaugural')\n",
    "inaugural.fileids()\n",
    "[fileid[:4] for fileid in inaugural.fileids()][:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquez que l'année de chaque texte apparaît dans son nom de fichier. Pour obtenir l'année à partir du nom de fichier, nous avons extrait les quatre premiers caractères, en utilisant `fileid[:4]`.\n",
    "\n",
    "Examinons comment les mots *America* et *citizen* sont utilisés au fil du temps. Le code suivant convertit les mots dans le corpus inaugural en minuscules en utilisant `w.lower()` [1], puis vérifie s'ils commencent par l'un des \"cibles\" `america` ou `citizen` en utilisant `startswith()` [1]. Il comptera donc les mots comme *American's* et *Citizens*. Nous apprendrons à propos des distributions de fréquence conditionnelles dans 2 ; pour l'instant, considérons simplement la sortie, montrée dans 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in inaugural.words(fileid)\n",
    "    for target in ['america', 'citizen']\n",
    "    if w.lower().startswith(target))\n",
    "cfd.plot()\n",
    "display(cfd) # pb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1.1: *Graphique d'une distribution de fréquence conditionnelle : tous les mots dans le corpus de discours inauguraux qui commencent par america ou citizen sont comptés ; des comptes séparés sont gardés pour chaque discours ; ils sont tracés de sorte que les tendances dans l'utilisation au fil du temps puissent être observées ; les comptes ne sont pas normalisés pour la longueur du document.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Corpus de textes annotés\n",
    "\n",
    "De nombreux corpus de textes contiennent des annotations linguistiques, représentant des tags morphosyntaxiques, des entités nommées, des structures syntaxiques, des rôles sémantiques, etc. NLTK fournit des moyens pratiques pour accéder à plusieurs de ces corpus, et possède des paquets de données contenant des corpus et des échantillons de corpus, téléchargeables gratuitement pour une utilisation en enseignement et en recherche. La Table 1.2 liste certains des corpus. Pour des informations sur leur téléchargement, consultez http://nltk.org/data. Pour davantage d'exemples sur l'accès aux corpus NLTK, veuillez consulter le HOWTO Corpus à http://nltk.org/howto.\n",
    "\n",
    "Tableau 1.2:\n",
    "\n",
    "Certains des corpus et échantillons de corpus distribués avec NLTK : pour des informations sur leur téléchargement et utilisation, veuillez consulter le site web NLTK.\n",
    "\n",
    "solé pour la confusion. Voici la traduction de la table en format markdown :\n",
    "Corpus\tCompilateur\tContenus\n",
    "Corpus inaugural\tCSpan\tDiscours inauguraux présidentiels américains (1789-présent)\n",
    "Corpus POS-taggué indien\tKumaran et al\t60k mots, taggués (bengali, hindi, marathi, telugu)\n",
    "Corpus MacMorpho\tNILC, USP, Brésil\t1M de mots, taggués (portugais brésilien)\n",
    "\n",
    "|Corpus|Compilateur|Contenus|\n",
    "|------|-----------|--------|\n",
    "|Corpus inaugural|CSpan|Discours inauguraux présidentiels des États-Unis (1789-présent)|\n",
    "|Corpus POS-tagué indien|Kumaran et al|60k mots, taggés (Bangla, Hindi, Marathi, Telugu)|\n",
    "|Corpus MacMorpho|NILC, USP, Brésil|1M mots, taggés (portugais brésilien)|\n",
    "|Critiques de films|Pang, Lee|2k critiques de films avec classification de la polarité de l'avis|\n",
    "|Corpus de noms|Kantrowitz, Ross|8k noms masculins et féminins|\n",
    "|NIST 1999 Info Extr (sélections)|Garofolo|63k mots, marquage SGML de nouvelles et d'entités nommées|\n",
    "|Nombank|Meyers|115k propositions, 1400 cadres nominaux|\n",
    "|Corpus NPS Chat|Forsyth, Martell|10k messages de chat IM, taggés POS et dialogue-act|\n",
    "|Open Multilingual WordNet|Bond et al|15 langues, alignées sur WordNet anglais|\n",
    "|Corpus PP Attachment|Ratnaparkhi|28k phrases prépositionnelles, taggées comme modificateurs nominaux ou verbaux|\n",
    "|Proposition Bank|Palmer|113k propositions, 3300 cadres verbeux|\n",
    "|Classification des questions|Li, Roth|6k questions, catégorisées|\n",
    "|Corpus Reuters|Reuters|1.3M mots, 10k documents de nouvelles, catégorisés|\n",
    "|Thesaurus de Roget|Projet Gutenberg|200k mots, formatés|\n",
    "|RTE Textual Entailment|Dagan et al|8k paires de phrases, catégorisées|\n",
    "|SEMCOR|Rus, Mihalcea|880k mots, taggés pour le part-of-speech et le sens|\n",
    "|Corpus Senseval 2|Pedersen|600k mots, taggés pour le part-of-speech et le sens|\n",
    "|SentiWordNet|Esuli, Sebastiani|scores de sentiment pour 145k ensembles de synonymes WordNet|\n",
    "|textes de Shakespeare (sélections)|Bosak|8 livres en format XML|\n",
    "|Corpus de l'état de l'Union|CSPAN|485k mots, formatés|\n",
    "|Corpus de mots vides|Porter et al|2,400 mots vides pour 11 langues|\n",
    "|Corpus Swadesh|Wiktionary|listes comparatives de mots pour 24 langues|\n",
    "|Switchboard Corpus (selections)|LDC|36 conversations téléphoniques, transcrites et analysées|\n",
    "|Univ Decl of Human Rights|Organisation des Nations Unies|480k mots, plus de 300 langues|\n",
    "|Penn Treebank (selections)|LDC|40k mots, taggés et analysés|\n",
    "|TIMIT Corpus (selections)|NIST/LDC|Fichiers audio et transcriptions pour 16 locuteurs|\n",
    "|VerbNet 2.1|Palmer et al|5k verbes, organisés hiérarchiquement, liés à WordNet|\n",
    "|Wordlist Corpus|OpenOffice.org et al|960k mots et 20k affixes pour 8 langues|\n",
    "|WordNet 3.0 (anglais)|Miller, Fellbaum|145k ensembles de synonymes|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Corpus dans d'autres langues\n",
    "\n",
    "NLTK est livré avec des corpus pour de nombreuses langues, bien que dans certains cas vous devrez apprendre à manipuler les encodages de caractères en Python avant d'utiliser ces corpus (voir 3.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('cess_esp')\n",
    "# nltk.corpus.cess_esp.words()\n",
    "# ['El', 'grupo', 'estatal', 'Electricit\\xe9_de_France', ...]\n",
    "# nltk.download('floresta')\n",
    "nltk.corpus.floresta.words()\n",
    "# ['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]\n",
    "# nltk.download('indian')\n",
    "nltk.corpus.indian.words('hindi.pos')\n",
    "# ['पूर्ण', 'प्रतिबंध', 'हटाओ', ':', 'इराक', 'संयुक्त', ...]\n",
    "# nltk.download('udhr')\n",
    "nltk.corpus.udhr.fileids()\n",
    "# ['Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',\n",
    "# 'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',\n",
    "# 'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...]\n",
    "nltk.corpus.udhr.words('Javanese-Latin1')[11:]\n",
    "# ['Saben', 'umat', 'manungsa', 'lair', 'kanthi', 'hak', ...]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dernier de ces corpus, `udhr`, contient la Déclaration universelle des droits de l'homme dans plus de 300 langues. Les identifiants de fichier pour ce corpus incluent des informations sur l'encodage de caractères utilisé dans le fichier, comme `UTF8` ou `Latin1`. Utilisons une distribution de fréquence conditionnelle pour examiner les différences dans les longueurs des mots pour une sélection de langues incluses dans le corpus `udhr`. La sortie est affichée dans 1.2 (exécutez le programme vous-même pour voir un graphique en couleurs). Notez que `True` et `False` sont les valeurs booléennes intégrées de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
    "    'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (lang, len(word))\n",
    "    for lang in languages\n",
    "    for word in udhr.words(lang + '-Latin1'))\n",
    "cfd.plot(cumulative=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1.2 : *Distributions cumulatives de longueur de mots : Six traductions de la Déclaration universelle des droits de l'homme sont traitées ; ce graphique montre que les mots ayant 5 lettres ou moins représentent environ 80 % du texte en ibibio, 60 % du texte en allemand et 25 % du texte en inuktitut.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A vous de jouer** : Choisissez une langue qui vous intéresse dans `udhr.fileids()`, et définissez une variable `raw_text = udhr.raw(Language-Latin1)`. Maintenant, tracez une distribution de fréquence des lettres du texte en utilisant `nltk.FreqDist(raw_text).plot()`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malheureusement, pour de nombreuses langues, des corpus importants ne sont pas encore disponibles. Souvent, il n'y a pas de soutien gouvernemental ou industriel pour développer des ressources linguistiques, et les efforts individuels sont épars et difficiles à découvrir ou à réutiliser. Certaines langues n'ont pas de système d'écriture établi, ou sont en danger. (Voir 7 pour des suggestions sur la façon de localiser des ressources linguistiques.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les corpus de SKL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. Le jeu de données textuelles 20 newsgroups\n",
    "\n",
    "[fetch_20newsgroups]: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
    "[CountVectorizer]: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "[fetch_20newsgroups_vectorized]: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html\n",
    "\n",
    "Le jeu de données 20 newsgroups comprend environ 18 000 publications de groupes de discussion sur 20 sujets divisés en deux sous-ensembles: un pour l'entraînement (ou le développement) et l'autre pour les tests (ou pour l'évaluation des performances). La séparation entre l'ensemble d'entraînement et l'ensemble de test est basée sur des messages publiés avant et après une date spécifique.\n",
    "\n",
    "Ce module contient deux chargeurs. Le premier, [**`sklearn.datasets.fetch_20newsgroups`**][fetch_20newsgroups], renvoie une liste des textes bruts qui peuvent alimenter  des extracteurs de caractéristiques de texte tels que [**`CountVectorizer`**][CountVectorizer] avec des paramètres personnalisés pour extraire des vecteurs de caractéristiques. Le second, [**`sklearn.datasets.fetch_20newsgroups_vectorized`**][fetch_20newsgroups_vectorized], renvoie des caractéristiques prêtes à l'emploi, c'est-à-dire qu'il n'est pas nécessaire d'utiliser un extracteur de caractéristiques.\n",
    "\n",
    "Caractéristiques du jeu de données :\n",
    "\n",
    "* Classes : 20\n",
    "* Nombre d'échantillons : 18 846\n",
    "* Dimensionalité : 1\n",
    "* Caractéristiques : text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2.1. Utilisation\n",
    "\n",
    "[fetch_20newsgroups]: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
    "[load_files]: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html\n",
    "\n",
    "La fonction [**`sklearn.datasets.fetch_20newsgroups`**][fetch_20newsgroups] est une fonction de récupération / mise en cache de données qui télécharge l'archive de données du site 20 newsgroups d'origine, extrait le contenu de l'archive dans le dossier `~/scikit_learn_data/20news_home` et appelle [**`sklearn.datasets.load_files`**][load_files] sur l'ensemble d'entraînement ou de test, ou les deux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données réelles se trouvent dans les attributs `filename` et `target`. L'attribut cible est l'index entier de la catégorie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.filenames.shape\n",
    "# (11314,)\n",
    "newsgroups_train.target.shape\n",
    "# (11314,)\n",
    "newsgroups_train.target[:10]\n",
    "# array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de charger seulement une sous-sélection des catégories en passant la liste des catégories à charger à la fonction [**`sklearn.datasets.fetch_20newsgroups`**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = ['alt.atheism', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "list(newsgroups_train.target_names)\n",
    "# ['alt.atheism', 'sci.space']\n",
    "newsgroups_train.filenames.shape\n",
    "# (1073,)\n",
    "newsgroups_train.target.shape\n",
    "# (1073,)\n",
    "newsgroups_train.target[:10]\n",
    "# array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertir le texte en vecteurs\n",
    "\n",
    "Afin d'alimenter des modèles prédictifs ou de clustering avec les données textuelles, il est nécessaire de convertir le texte en vecteurs de valeurs numériques adaptés pour l'analyse statistique. Cela peut être réalisé avec les utilitaires de `sklearn.feature_extraction.text`, comme le montre l'exemple suivant qui extrait des vecteurs [**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) d'unigrammes d'un sous-ensemble de 20news :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 34118)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape\n",
    "# (2034, 34118)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les vecteurs TF-IDF extraits sont très creux, avec une moyenne de 159 composants non-nuls par échantillon dans un espace de plus de 30000 dimensions (moins de 0,5 % de caractéristiques non-nulles) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159.0132743362832"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`sklearn.datasets.fetch_20newsgroups_vectorized`**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html) est une fonction qui retourne des fonctionnalités de comptage de jetons prêtes à l'emploi au lieu de noms de fichiers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2.3. Filtrer le texte pour un entraînement plus réaliste\n",
    "\n",
    "Il est facile pour un classificateur de surapprendre sur des éléments particuliers qui apparaissent dans les données 20 Newsgroups, tels que les en-têtes de groupes de nouvelles. De nombreux classificateurs obtiennent des scores F très élevés, mais leurs résultats ne se généraliseraient pas à d'autres documents qui ne sont pas de cette période de temps.\n",
    "\n",
    "Par exemple, examinons les résultats d'un classificateur Naive Bayes multinomial, qui est rapide à entraîner et qui obtient un score F décent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8821359240272957"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
    "\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
    "# 0.88213..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(L'exemple [**Classification de documents de textes en utilisant des caractéristiques creuses**](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html) mélange les données d'entraînement et de test, au lieu de les segmenter par le temps, et dans ce cas le classificateur Naive Bayes multinomial obtient un score F beaucoup plus élevé de 0,88. Etes-vous déjà suspicieux de ce qui se passe à l'intérieur de ce classificateur ?)\n",
    "\n",
    "Examinons ce que sont les caractéristiques les plus informatives :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: edu it and in you that is of to the\n",
      "comp.graphics: edu in graphics it is for and of to the\n",
      "sci.space: edu it that is in and space to of the\n",
      "talk.religion.misc: not it you in is that and to of the\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.feature_count_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
    "# alt.atheism: edu it and in you that is of to the\n",
    "# comp.graphics: edu in graphics it is for and of to the\n",
    "# sci.space: edu it that is in and space to of the\n",
    "# talk.religion.misc: not it you in is that and to of the"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant voir de nombreuses choses que ces caractéristiques ont surappris :\n",
    "* Presque chaque groupe est distingué par la présence ou non de têtes telles que `NNTP-Posting-Host:` et `Distribution:`.\n",
    "* Une autre caractéristique significative concerne l'affiliation de l'expéditeur à une université, indiquée soit par leurs en-têtes, soit par leur signature.\n",
    "* Le mot \"article\" est une caractéristique significative, en fonction de la fréquence avec laquelle les gens citent des messages précédents de cette façon: \"Dans l'article [ID de l'article], [nom] <[adresse e-mail]> a écrit:\".\n",
    "* D'autres caractéristiques correspondent aux noms et aux adresses e-mail de personnes particulières qui ont publié à ce moment-là.\n",
    "\n",
    "Avec une telle abondance d'indices qui distinguent les groupes de nouvelles, les classificateurs n'ont presque pas besoin d'identifier les sujets à partir du texte et ils performent tous à un niveau élevé.\n",
    "\n",
    "Pour cette raison, les fonctions qui chargent les données 20 Newsgroups fournissent un paramètre appelé **remove**, lui indiquant quelles informations enlever de chaque fichier. **remove** devrait être un tuple contenant n'importe quel sous-ensemble de `('headers', 'footers', 'quotes')`, indiquant qu'il faut supprimer les en-têtes, les blocs de signature et les blocs de citation respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7731035068127478"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    categories=categories\n",
    ")\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
    "# 0.77310..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce classificateur a perdu une grande partie de son score F simplement parce que nous avons enlevé des métadonnées qui n'ont peu à voir avec la classification de sujets. Il perd encore plus si nous enlevons également ces métadonnées des données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(alpha=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(alpha=0.01)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(alpha=0.01)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    categories=categories\n",
    ")\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7699517518452172"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
    "# 0.76995..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'autres classificateurs font mieux face à cette version plus difficile de la tâche. Essayez l'exemple de [**pipeline d'extraction et d'évaluation des caractéristiques de texte**](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_text_feature_extraction.html) avec et sans l'option `remove` pour comparer les résultats."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considérations de données\n",
    "\n",
    "Les Cleveland Indians sont une équipe de baseball de la ligue majeure basée à Cleveland, Ohio, États-Unis. En décembre 2020, il a été rapporté que « Après plusieurs mois de discussion déclenchée par la mort de George Floyd et un bilan national sur la race et le colonialisme, les Cleveland Indians ont décidé de changer de nom ». Le propriétaire de l'équipe, Paul Dolan, a « clairement fait comprendre que l'équipe ne fera pas de son surnom informel - le Tribe - son nouveau nom d'équipe ». « Ce ne sera pas un demi-pas loin des Indiens », a déclaré Dolan. « Nous n'aurons pas de nom à thème amérindien ».\n",
    "\n",
    "https://www.mlb.com/news/cleveland-indians-team-name-change\n",
    "\n",
    "#### Recommandation\n",
    "\n",
    "* Lors de l'évaluation de classificateurs de texte sur les données 20 Newsgroups, vous devriez enlever les métadonnées liées aux groupes de nouvelles. Dans scikit-learn, vous pouvez le faire en définissant `remove=('headers', 'footers', 'quotes')`. Le score F sera plus bas parce qu'il est plus réaliste.\n",
    "* Cet ensemble de données de texte contient des données qui peuvent être inappropriées pour certaines applications NLP. Un exemple est indiqué dans la section \"Considérations de données\" ci-dessus. Le défi de l'utilisation de jeux de données de texte actuels en NLP pour des tâches telles que la complétion de phrases, le regroupement et d'autres applications est que le texte biaisé et inflammatoire culturel se propagera. Cela devrait être pris en compte lors de l'utilisation du jeu de données, de la revue de la sortie et du biais devrait être documenté.\n",
    "\n",
    "#### Exemples\n",
    "\n",
    "**Pipeline d'échantillon pour l'extraction et l'évaluation des caractéristiques de texte**\n",
    "\n",
    "**Classification de documents de texte à l'aide de caractéristiques creuses**\n",
    "\n",
    "**Comparaison FeatureHasher et DictVectorizer**\n",
    "\n",
    "##### [**Regroupement de documents texte à l'aide de k-means**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/text/plot_document_clustering.ipynb)<br/>([*Clustering text documents using k-means*](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les corpus de Spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les exemples Kaggle de référence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autres corpus d'exemples standards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mes propres corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
